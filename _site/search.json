[
  {
    "objectID": "workflows/supervised_learning.html",
    "href": "workflows/supervised_learning.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised ML workflow for building a classification model on data with categorical and continuous features.\nUses the penguins dataset, and trains a random forest to predict the penguin species. Uses scikit-learn for pre-processing, modeling, and evaluation.\n\nIncludes stratified train/test split\nIncludes imputation inside the pipeline\nHandles categorical and numerical features separately\nRuns a grid search to find best model parameters\nEvaluates results using both classification report and confusion matrix\nExtracts feature importances with proper naming\n\n\nfrom IPython.display import display\nimport seaborn as sns\nimport pandas as pd\nfrom minieda import summarize # to install from github: pip install git+https://github.com/dbolotov/minieda.git\nimport time\nimport pprint\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load dataset and display first few rows\ndf = sns.load_dataset(\"penguins\")\n\nprint(\"\\n----- Script Output -----\\n\")\n\ndisplay(df.head())\n\n# Display summary\ndisplay(summarize(df))\n\n# Per-class value count\nprint(\"\\n----- Target class frequencies (normalized) -----\\n\")\nprint(df['species'].value_counts(normalize=True).rename(None).rename_axis(None))\n\n# Define columns\ncat_cols = ['island', 'sex']\nnum_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n\n# Drop rows where the target is missing (can't model without target)\ndf = df.dropna(subset=['species'])\n\n# Split the data\nX = df[cat_cols + num_cols]\ny = df['species']\n\n# Split into training and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Define preprocessing for numeric and categorical features\nnumeric_preprocessing = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler())\n])\n\ncategorical_preprocessing = Pipeline([\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('encode', OneHotEncoder(drop='first', sparse_output=False))  # one-hot; drop first feature to avoid multicollinearity\n])\n\n# Combine into a column transformer\npreprocessor = ColumnTransformer([\n    ('num', numeric_preprocessing, num_cols),\n    ('cat', categorical_preprocessing, cat_cols)\n])\n\n# Base pipeline (model will be tuned below)\nclf_pipeline = Pipeline([\n    ('pre', preprocessor),\n    ('model', RandomForestClassifier(random_state=42))\n])\n\n# Define hyperparameter grid\nparam_grid = {\n    'model__n_estimators': [20,30,40],\n    'model__max_depth': [None],\n    'model__min_samples_leaf': [1, 3, 5, 7],\n    'model__max_features': ['sqrt']\n}\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    estimator=clf_pipeline,\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=2\n)\n\n# Fit grid search on training data\nprint(\"\\n----- GRID SEARCH -----\\n\")\nstart_time = time.time()\ngrid_search.fit(X_train, y_train)\nprint(f\"\\nGrid search completed in {time.time() - start_time:.2f} seconds\")\n\nprint(\"\\n----- Best Grid Search Result -----\")\nprint(f\"Accuracy: {grid_search.best_score_:.4f} ± {grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f}\")\nprint(\"Parameters:\")\npprint.pprint(grid_search.best_params_)\n\n# Use best model from grid search\nclf_pipeline = grid_search.best_estimator_\n\n# Evaluate\nprint(\"\\n----- EVALUATION -----\")\nprint(\"\\n----- Train/Test Accuracy -----\\n\")\nprint(f\"Train accuracy: {clf_pipeline.score(X_train, y_train):.4f}\")\nprint(f\"Test accuracy:  {clf_pipeline.score(X_test, y_test):.4f}\")\n\ny_test_pred = clf_pipeline.predict(X_test)\nprint(\"\\n----- Classification Report -----\\n\")\nprint(classification_report(y_test, y_test_pred))\nprint(\"\\n----- Confusion Matrix -----\\n\")\ncm = confusion_matrix(y_test, y_test_pred, labels=clf_pipeline.classes_)\nprint(cm)\n\n# Print normalized feature importances\nmodel = clf_pipeline.named_steps['model']\nencoded_feature_names = clf_pipeline.named_steps['pre'].get_feature_names_out()\n\nfeat_importance_df = pd.DataFrame({\n    'feature': encoded_feature_names,\n    'importance': model.feature_importances_\n}).sort_values(by='importance', ascending=False)\n\nprint(\"\\n----- Feature Importance -----\\n\")\nprint(feat_importance_df)\n\n\n----- Script Output -----\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndtype\ncount\nunique\nunique_perc\nmissing\nmissing_perc\nzero\nzero_perc\ntop\nfreq\nmean\nstd\nmin\n50%\nmax\nskew\n\n\n\n\nbill_length_mm\nfloat64\n342\n164\n47.67\n2\n0.58\n0\n0.0\n\n\n43.92\n5.46\n32.1\n44.45\n59.6\n0.05\n\n\nbill_depth_mm\nfloat64\n342\n80\n23.26\n2\n0.58\n0\n0.0\n\n\n17.15\n1.97\n13.1\n17.3\n21.5\n-0.14\n\n\nflipper_length_mm\nfloat64\n342\n55\n15.99\n2\n0.58\n0\n0.0\n\n\n200.92\n14.06\n172.0\n197.0\n231.0\n0.35\n\n\nbody_mass_g\nfloat64\n342\n94\n27.33\n2\n0.58\n0\n0.0\n\n\n4201.75\n801.95\n2700.0\n4050.0\n6300.0\n0.47\n\n\nspecies\nobject\n344\n3\n0.87\n0\n0.00\n0\n0.0\nAdelie\n152\n\n\n\n\n\n\n\n\nisland\nobject\n344\n3\n0.87\n0\n0.00\n0\n0.0\nBiscoe\n168\n\n\n\n\n\n\n\n\nsex\nobject\n333\n2\n0.58\n11\n3.20\n0\n0.0\nMale\n168\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n----- Target class frequencies (normalized) -----\n\nAdelie       0.441860\nGentoo       0.360465\nChinstrap    0.197674\ndtype: float64\n\n----- GRID SEARCH -----\n\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n\nGrid search completed in 5.10 seconds\n\n----- Best Grid Search Result -----\nAccuracy: 0.9855 ± 0.0073\nParameters:\n{'model__max_depth': None,\n 'model__max_features': 'sqrt',\n 'model__min_samples_leaf': 1,\n 'model__n_estimators': 30}\n\n----- EVALUATION -----\n\n----- Train/Test Accuracy -----\n\nTrain accuracy: 1.0000\nTest accuracy:  1.0000\n\n----- Classification Report -----\n\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n----- Confusion Matrix -----\n\n[[30  0  0]\n [ 0 14  0]\n [ 0  0 25]]\n\n----- Feature Importance -----\n\n                  feature  importance\n0     num__bill_length_mm    0.312770\n1      num__bill_depth_mm    0.216133\n2  num__flipper_length_mm    0.205913\n4       cat__island_Dream    0.138414\n3        num__body_mass_g    0.096331\n5   cat__island_Torgersen    0.021391\n6           cat__sex_Male    0.009048",
    "crumbs": [
      "Workflows",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\nA collection of useful resources and tools.\n\nOnline Books\n\n\n\n\nCategory\nResource\nDescription\n\n\n\n\nExplainability\nInterpretable Machine Learning\nA practical overview of techniques for making ML models more transparent, including SHAP.\n\n\nVisualization\nUW Interactive Data Lab Curriculum\nBook on statistical visualization using Vega-Lite and Altair.\n\n\nVisualization\nFundamentals of Data Visualization\nPrinciples and examples of clear, effective visual communication.\n\n\nTime Series\nForecasting: Principles and Practice\nCovers forecasting techniques like exponential smoothing and ARIMA, with examples in R.\n\n\nData Imputation\nFlexible Imputation of Missing Data\nMethods to handle missing data, with emphasis on multiple imputation.\n\n\nFraud Detection\nFraud Detection Handbook\nApplied techniques for detecting fraud in highly imbalanced datasets. Covers using a fraud data simulator.\n\n\n\n\n\n\nTools\n\nSDV - Python library for creating tabular synthetic data.\n\n\n\nResources used to make this guide\n\nQuarto: Publishing system. Supports notebooks, markdown, citations, and publishing to HTML and PDF.\nbootswatch: Collection of free themes for Bootstrap-based sites (used to style this book).",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nExplainability\nOversampling\nOverfitting and Underfitting\nData leakage\nFeatures\nReinforcement learning\nSupervised learning\nUnsupervised learning\nDescriptive statistics\nDeep learning\nDistribution\nProbability",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "common_problems.html",
    "href": "common_problems.html",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nNot enough data\nUse data augmentation or synthetic sampling (e.g. SMOTE, SDV)\n\n\nData not representative of the distribution\nReassess how data was collected; consider stratified sampling\n\n\nImbalanced classes\nTry resampling, adjusting class weights, or adding synthetic data with SDV\n\n\nToo much data (examples)\nSubsample or use mini-batch training; profile before full-scale training\n\n\nToo many features / high dimensionality\nApply feature selection or dimensionality reduction (e.g. PCA)\n\n\nData has extreme values, outliers, or anomalies\nUse robust statistics, or find such values using outlier/anomaly detection methods. Consider removing examples.\n\n\nData may have been faked\nCheck for duplicate rows, unnatural distributions, and value repetition\n\n\nData leakage\nReview data sources and pipeline; make sure target isn’t leaking into features\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nModel performs well on training, terrible on test (overfitting)\nReduce model complexity, add regularization, or get more data\n\n\nModel performs poorly on training AND test data (underfitting)\nUse a more complex model, add better features, reduce regularization.\n\n\nClassification model worse than a random guess or worse than majority class guess\nInvestigate data quality, imbalanced classes.\n\n\nModel performs unusually well on train and test data\nCheck for data leakage; the model may have access to information it shouldn’t.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nJupyter notebook is too large\nAvoid storing large Plotly outputs; clean outputs or split the notebook\n\n\nModel training takes too long\nUse smaller subsets for tuning; simplify the model or parallelize training",
    "crumbs": [
      "Common Problems in DS"
    ]
  },
  {
    "objectID": "common_problems.html#data",
    "href": "common_problems.html#data",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nNot enough data\nUse data augmentation or synthetic sampling (e.g. SMOTE, SDV)\n\n\nData not representative of the distribution\nReassess how data was collected; consider stratified sampling\n\n\nImbalanced classes\nTry resampling, adjusting class weights, or adding synthetic data with SDV\n\n\nToo much data (examples)\nSubsample or use mini-batch training; profile before full-scale training\n\n\nToo many features / high dimensionality\nApply feature selection or dimensionality reduction (e.g. PCA)\n\n\nData has extreme values, outliers, or anomalies\nUse robust statistics, or find such values using outlier/anomaly detection methods. Consider removing examples.\n\n\nData may have been faked\nCheck for duplicate rows, unnatural distributions, and value repetition\n\n\nData leakage\nReview data sources and pipeline; make sure target isn’t leaking into features",
    "crumbs": [
      "Common Problems in DS"
    ]
  },
  {
    "objectID": "common_problems.html#modeling",
    "href": "common_problems.html#modeling",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nModel performs well on training, terrible on test (overfitting)\nReduce model complexity, add regularization, or get more data\n\n\nModel performs poorly on training AND test data (underfitting)\nUse a more complex model, add better features, reduce regularization.\n\n\nClassification model worse than a random guess or worse than majority class guess\nInvestigate data quality, imbalanced classes.\n\n\nModel performs unusually well on train and test data\nCheck for data leakage; the model may have access to information it shouldn’t.",
    "crumbs": [
      "Common Problems in DS"
    ]
  },
  {
    "objectID": "common_problems.html#workflow",
    "href": "common_problems.html#workflow",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nJupyter notebook is too large\nAvoid storing large Plotly outputs; clean outputs or split the notebook\n\n\nModel training takes too long\nUse smaller subsets for tuning; simplify the model or parallelize training",
    "crumbs": [
      "Common Problems in DS"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Quick Reference",
    "section": "",
    "text": "Data Science Quick Reference\nThis site is a quick-access resource for data science, machine learning, and related topics, with examples in Python. It’s built for students and professionals who want a practical summary of key ideas, workflows, and tools.\nThe content emphasizes short reminders, real-world examples, and curated links. Use it when:\n\nYou need a refresher on a concept you’ve already learned.\n\nYou’re in the middle of a project and want to check best practices.\n\nYou’re deciding which metrics fit a specific ML task.\n\nYou’re looking for quick access to reliable external resources.\n\nIt’s a work in progress and will grow over time.\nThe approach is generalist, reflecting common patterns seen in real-world projects. Tools and methods vary across teams, but this reference focuses on Python, with pandas and scikit-learn at the core.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "workflows/index.html",
    "href": "workflows/index.html",
    "title": "Workflows Index",
    "section": "",
    "text": "Workflows Index\nThis section includes end-to-end Jupyter Notebooks with examples of common data science workflows.\n\nSupervised learning (classification) - Training a random forest classifier with scikit-learn preprocessing and grid search on the penguins dataset.",
    "crumbs": [
      "Workflows",
      "Workflows Index"
    ]
  }
]