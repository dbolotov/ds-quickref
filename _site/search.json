[
  {
    "objectID": "workflows/index.html",
    "href": "workflows/index.html",
    "title": "Workflows Index",
    "section": "",
    "text": "Workflows Index\nThis section contains end-to-end Jupyter Notebooks with examples of common data science workflows.\n\nClassification with Tabular Data - Training a random forest classifier with scikit-learn preprocessing and grid search on the Palmer penguins dataset.",
    "crumbs": [
      "Workflows",
      "Workflows Index"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\nA collection of useful resources and tools.\n\nOnline Books\n\n\n\n\nCategory\nResource\nDescription\n\n\n\n\nExplainability\nInterpretable Machine Learning\nA practical overview of techniques for making ML models more transparent, including SHAP.\n\n\nVisualization\nUW Interactive Data Lab Curriculum\nBook on statistical visualization using Vega-Lite and Altair.\n\n\nVisualization\nFundamentals of Data Visualization\nPrinciples and examples of clear, effective visual communication.\n\n\nTime Series\nForecasting: Principles and Practice\nCovers forecasting techniques like exponential smoothing and ARIMA, with examples in R.\n\n\nData Imputation\nFlexible Imputation of Missing Data\nMethods to handle missing data, with emphasis on multiple imputation.\n\n\nFraud Detection\nFraud Detection Handbook\nApplied techniques for detecting fraud in highly imbalanced datasets. Covers using a fraud data simulator.\n\n\n\n\n\n\nTools\n\nSDV - Python library for creating tabular synthetic data.\npermetrics - Python library for performance metrics of machine learning models. Documentation site includes quick explanations of each metric.\n\n\n\nResources used to make this guide\n\nQuarto: Extensive publishing system. Supports jupyter notebooks and markdown.\nbootswatch: Collection of free themes for Bootstrap-based sites.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Quick Reference",
    "section": "",
    "text": "Data Science Quick Reference\nThis site is a quick-access resource for data science, machine learning, and related topics, with examples in Python. It’s built for students and professionals who want a practical summary of key ideas, workflows, and tools.\nThe content emphasizes short reminders, real-world examples, and curated links. Use it when:\n\nYou need a refresher on a concept you’ve already learned.\n\nYou’re in the middle of a project and want to check best practices.\n\nYou’re deciding which metrics fit a specific ML task.\n\nYou’re looking for quick access to reliable external resources.\n\nIt’s a work in progress and will grow over time.\nThe approach is generalist, reflecting common patterns seen in real-world projects. Tools and methods vary across teams, but this reference focuses on Python, with pandas and scikit-learn at the core.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "common_problems.html",
    "href": "common_problems.html",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nNot enough data\nUse data augmentation or synthetic sampling (e.g. SMOTE, SDV)\n\n\nData not representative of the distribution\nReassess how data was collected; consider stratified sampling\n\n\nImbalanced classes\nTry resampling, adjusting class weights, or adding synthetic data with SDV\n\n\nToo much data (examples)\nSubsample or use mini-batch training; profile before full-scale training\n\n\nToo many features / high dimensionality\nApply feature selection or dimensionality reduction (e.g. PCA)\n\n\nData has extreme values, outliers, or anomalies\nUse robust statistics, or find such values using outlier/anomaly detection methods. Consider removing examples.\n\n\nData may have been faked\nCheck for duplicate rows, unnatural distributions, and value repetition\n\n\nData leakage\nReview data sources and pipeline; make sure target isn’t leaking into features\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nModel performs well on training, terrible on test (overfitting)\nReduce model complexity, add regularization, or get more data\n\n\nModel performs poorly on training AND test data (underfitting)\nUse a more complex model, add better features, reduce regularization.\n\n\nClassification model worse than a random guess or worse than majority class guess\nInvestigate data quality, imbalanced classes.\n\n\nModel performs unusually well on train and test data\nCheck for data leakage; the model may have access to information it shouldn’t.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nJupyter notebook is too large\nAvoid storing large Plotly outputs; clean outputs or split the notebook\n\n\nModel training takes too long\nUse smaller subsets for tuning; simplify the model or parallelize training",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "common_problems.html#data",
    "href": "common_problems.html#data",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nNot enough data\nUse data augmentation or synthetic sampling (e.g. SMOTE, SDV)\n\n\nData not representative of the distribution\nReassess how data was collected; consider stratified sampling\n\n\nImbalanced classes\nTry resampling, adjusting class weights, or adding synthetic data with SDV\n\n\nToo much data (examples)\nSubsample or use mini-batch training; profile before full-scale training\n\n\nToo many features / high dimensionality\nApply feature selection or dimensionality reduction (e.g. PCA)\n\n\nData has extreme values, outliers, or anomalies\nUse robust statistics, or find such values using outlier/anomaly detection methods. Consider removing examples.\n\n\nData may have been faked\nCheck for duplicate rows, unnatural distributions, and value repetition\n\n\nData leakage\nReview data sources and pipeline; make sure target isn’t leaking into features",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "common_problems.html#modeling",
    "href": "common_problems.html#modeling",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nModel performs well on training, terrible on test (overfitting)\nReduce model complexity, add regularization, or get more data\n\n\nModel performs poorly on training AND test data (underfitting)\nUse a more complex model, add better features, reduce regularization.\n\n\nClassification model worse than a random guess or worse than majority class guess\nInvestigate data quality, imbalanced classes.\n\n\nModel performs unusually well on train and test data\nCheck for data leakage; the model may have access to information it shouldn’t.",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "common_problems.html#workflow",
    "href": "common_problems.html#workflow",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nJupyter notebook is too large\nAvoid storing large Plotly outputs; clean outputs or split the notebook\n\n\nModel training takes too long\nUse smaller subsets for tuning; simplify the model or parallelize training",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nExplainability\nOversampling\nOverfitting and Underfitting\nData leakage\n\nExamples\n\nImputing missing values on the entire dataset before splitting into train and test.\n\n\nTrain, test, validation sets\nFeatures, columns, predictors\nTypes of learning\n\nSupervised learning\nUnsupervised learning\nDeep learning\nReinforcement learning\n\nDescriptive statistics\nDistribution\nProbability",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Data Science and Machine Learning Metrics",
    "section": "",
    "text": "Data Science and Machine Learning Metrics\nThis page is a quick reference for common metrics across tasks like classification, regression, and clustering. Each entry includes a definition, when to use it, and links to an explanation (mostly from Wikipedia) and the relevant scikit-learn doc. This list is not meant to be exhaustive.\n\nMetrics explained\nA metric is a number that measures model performance, or how well predictions match actual outcomes.\n\nIn supervised learning, metrics evaluate prediction quality (e.g. RMSE, F1).\nIn unsupervised learning, they assess structure or similarity (e.g. silhouette score).\nDuring training, metrics guide choices like model selection and early stopping.\n\n\nRelated terms\n\nLoss function: What the model optimizes during training.\nMetric: What you monitor to evaluate results.\nError metric: Often used in regression to describe prediction error.\n\n\n\n\nMetrics tables\n\nClassification\n\n\n\n\nMetric\nCode\nDetails\n\n\n\n\nAccuracy\nskl\nAccuracy. Proportion of correct predictions to total predictions. Simple and intuitive. Can be very misleading for imbalanced classes.\n\n\nPrecision\nskl\nPrecision. True Positives / (True Positives + False Positives). How many predicted positives are correct.\n\n\nRecall\nskl\nRecall. True Positives / (True Positives + False Negatives). How many actual positives were captured.\n\n\nF1\nskl\nF1 Score. Harmonic mean of precision and recall. Good for imbalanced classes.\n\n\nROC AUC\nskl\nROC AUC. Area under the ROC curve. Evaluates ranking performance across thresholds.\n\n\nPR AUC\nskl\nPR AUC. Area under the Precision-Recall curve. Better than ROC AUC for rare positives.\n\n\nLog Loss\nskl\nLogarithmic Loss. Penalizes confident wrong predictions. Common in probabilistic classifiers.\n\n\nBalanced Acc\nskl\nBalanced Accuracy. Mean recall across classes. Helps with imbalanced classes.\n\n\nMCC\nskl\nMatthews Correlation Coefficient Balanced score even for class imbalance. Based on confusion matrix.\n\n\n\n\n\n\nRegression\n\n\n\n\nMetric\nCode\nDetails\n\n\n\n\nMSE\nskl\nMean Squared Error. Average squared difference between predictions and true values. Penalizes larger errors more; sensitive to outliers. Not in original units.\n\n\nRMSE\nskl\nRoot Mean Squared Error. Same as MSE but in the original unit scale; easier to interpret. Still sensitive to outliers.\n\n\nMAE\nskl\nMean Absolute Error. Average absolute difference between predictions and actual values. More robust to outliers than MSE.\n\n\nR²\nskl\nCoefficient of Determination. Measures proportion of variance explained by the model. Can be negative.\n\n\nAdj R²\n\nAdjusted R². Like R² but penalizes for additional predictors. Helps avoid overfitting. Must be computed manually.\n\n\nMSLE\nskl\nMean Squared Log Error. MSE on log-transformed targets. Good for targets spanning orders of magnitude.\n\n\nMAPE\nskl\nMean Absolute Percentage Error. Average of absolute percentage errors. Can blow up if targets are near zero.\n\n\nSMAPE\n\nSymmetric MAPE. Like MAPE but less sensitive to small denominators. Often used in time series. Must implement manually.",
    "crumbs": [
      "Metrics"
    ]
  },
  {
    "objectID": "workflows/classification_tabular.html",
    "href": "workflows/classification_tabular.html",
    "title": "Classification with Tabular Data",
    "section": "",
    "text": "Supervised ML workflow for building a classification model on tabular data with categorical and continuous features.\nUses the penguins dataset, and trains a random forest to predict the penguin species. Uses scikit-learn for pre-processing, modeling, and evaluation.\n\nIncludes stratified train/test split\nIncludes imputation inside the pipeline\nHandles categorical and numerical features separately\nRuns a grid search to find best model parameters\nEvaluates results using both classification report and confusion matrix\nExtracts feature importances with proper naming\n\n\n\n\n\n\n\nNote\n\n\n\nEven though display(df.head()) and display(summarize(df)) commands appear at the top of the script, their output may appear at the bottom of the results due to how the script is processed during site rendering: html tables are deferred until the end. When you copy this code into a Jupyter notebook, outputs will show in order as expected.\n\n\n\nfrom IPython.display import display\nimport seaborn as sns\nimport pandas as pd\nfrom minieda import summarize # to install from github: pip install git+https://github.com/dbolotov/minieda.git\nimport time\nfrom pprint import pprint\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load dataset and display first few rows\ndf = sns.load_dataset(\"penguins\")\n\nprint(\"\\n----- Script Output -----\\n\")\n\ndisplay(df.head())\n\n# Display summary\ndisplay(summarize(df))\n\n# Per-class value count\nprint(\"\\n----- Target class frequencies (normalized) -----\\n\")\nprint(df['species'].value_counts(normalize=True).rename(None).rename_axis(None))\n\n# Define columns\ncat_cols = ['island', 'sex']\nnum_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n\n# Drop rows where the target is missing (can't model without target)\ndf = df.dropna(subset=['species'])\n\n# Split the data\nX = df[cat_cols + num_cols]\ny = df['species']\n\n# Split into training and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Define preprocessing for numeric and categorical features\nnumeric_preprocessing = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler())\n])\n\ncategorical_preprocessing = Pipeline([\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('encode', OneHotEncoder(drop='first', sparse_output=False))  # one-hot; drop first feature to avoid multicollinearity\n])\n\n# Combine into a column transformer\npreprocessor = ColumnTransformer([\n    ('num', numeric_preprocessing, num_cols),\n    ('cat', categorical_preprocessing, cat_cols)\n])\n\n# Base pipeline (model will be tuned below)\nclf_pipeline = Pipeline([\n    ('pre', preprocessor),\n    ('model', RandomForestClassifier(random_state=42))\n])\n\n# Define hyperparameter grid\nparam_grid = {\n    'model__n_estimators': [20,30,40],\n    'model__max_depth': [None],\n    'model__min_samples_leaf': [1, 3, 5, 7],\n    'model__max_features': ['sqrt']\n}\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    estimator=clf_pipeline,\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=2\n)\n\n# Fit grid search on training data\nprint(\"\\n----- GRID SEARCH -----\\n\")\nstart_time = time.time()\ngrid_search.fit(X_train, y_train)\nprint(f\"\\nGrid search completed in {time.time() - start_time:.2f} seconds\")\n\nprint(\"\\n----- Best Grid Search Result -----\")\nprint(f\"Accuracy: {grid_search.best_score_:.4f} ± {grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f}\")\nprint(\"Parameters:\")\npprint(grid_search.best_params_)\n\n# Use best model from grid search\nclf_pipeline = grid_search.best_estimator_\n\n# Evaluate\nprint(\"\\n----- EVALUATION -----\")\nprint(\"\\n----- Train/Test Accuracy -----\\n\")\nprint(f\"Train accuracy: {clf_pipeline.score(X_train, y_train):.4f}\")\nprint(f\"Test accuracy:  {clf_pipeline.score(X_test, y_test):.4f}\")\n\ny_test_pred = clf_pipeline.predict(X_test)\nprint(\"\\n----- Classification Report -----\\n\")\nprint(classification_report(y_test, y_test_pred))\nprint(\"\\n----- Confusion Matrix -----\\n\")\ncm = confusion_matrix(y_test, y_test_pred, labels=clf_pipeline.classes_)\nprint(cm)\n\n# Print normalized feature importances\nmodel = clf_pipeline.named_steps['model']\nencoded_feature_names = clf_pipeline.named_steps['pre'].get_feature_names_out()\n\nfeat_importance_df = pd.DataFrame({\n    'feature': encoded_feature_names,\n    'importance': model.feature_importances_\n}).sort_values(by='importance', ascending=False)\n\nprint(\"\\n----- Feature Importance -----\\n\")\nprint(feat_importance_df)\n\n\n----- Script Output -----\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndtype\ncount\nunique\nunique_perc\nmissing\nmissing_perc\nzero\nzero_perc\ntop\nfreq\nmean\nstd\nmin\n50%\nmax\nskew\n\n\n\n\nbill_length_mm\nfloat64\n342\n164\n47.67\n2\n0.58\n0\n0.0\n\n\n43.92\n5.46\n32.1\n44.45\n59.6\n0.05\n\n\nbill_depth_mm\nfloat64\n342\n80\n23.26\n2\n0.58\n0\n0.0\n\n\n17.15\n1.97\n13.1\n17.3\n21.5\n-0.14\n\n\nflipper_length_mm\nfloat64\n342\n55\n15.99\n2\n0.58\n0\n0.0\n\n\n200.92\n14.06\n172.0\n197.0\n231.0\n0.35\n\n\nbody_mass_g\nfloat64\n342\n94\n27.33\n2\n0.58\n0\n0.0\n\n\n4201.75\n801.95\n2700.0\n4050.0\n6300.0\n0.47\n\n\nspecies\nobject\n344\n3\n0.87\n0\n0.00\n0\n0.0\nAdelie\n152\n\n\n\n\n\n\n\n\nisland\nobject\n344\n3\n0.87\n0\n0.00\n0\n0.0\nBiscoe\n168\n\n\n\n\n\n\n\n\nsex\nobject\n333\n2\n0.58\n11\n3.20\n0\n0.0\nMale\n168\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n----- Target class frequencies (normalized) -----\n\nAdelie       0.441860\nGentoo       0.360465\nChinstrap    0.197674\ndtype: float64\n\n----- GRID SEARCH -----\n\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n\nGrid search completed in 4.59 seconds\n\n----- Best Grid Search Result -----\nAccuracy: 0.9855 ± 0.0073\nParameters:\n{'model__max_depth': None,\n 'model__max_features': 'sqrt',\n 'model__min_samples_leaf': 1,\n 'model__n_estimators': 30}\n\n----- EVALUATION -----\n\n----- Train/Test Accuracy -----\n\nTrain accuracy: 1.0000\nTest accuracy:  1.0000\n\n----- Classification Report -----\n\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n----- Confusion Matrix -----\n\n[[30  0  0]\n [ 0 14  0]\n [ 0  0 25]]\n\n----- Feature Importance -----\n\n                  feature  importance\n0     num__bill_length_mm    0.312770\n1      num__bill_depth_mm    0.216133\n2  num__flipper_length_mm    0.205913\n4       cat__island_Dream    0.138414\n3        num__body_mass_g    0.096331\n5   cat__island_Torgersen    0.021391\n6           cat__sex_Male    0.009048",
    "crumbs": [
      "Workflows",
      "Classification with Tabular Data"
    ]
  }
]