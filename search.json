[
  {
    "objectID": "common_problems.html",
    "href": "common_problems.html",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nNot enough data\nUse data augmentation or synthetic sampling (e.g. SMOTE, SDV)\n\n\nData not representative of the distribution\nReassess how data was collected; consider stratified sampling\n\n\nImbalanced classes\nTry resampling, adjusting class weights, or adding synthetic data with SDV\n\n\nToo much data (examples)\nSubsample or use mini-batch training; profile before full-scale training\n\n\nToo many features / high dimensionality\nApply feature selection or dimensionality reduction (e.g. PCA)\n\n\nData has extreme values, outliers, or anomalies\nUse robust statistics, or find such values using outlier/anomaly detection methods. Consider removing examples.\n\n\nData may have been faked\nCheck for duplicate rows, unnatural distributions, and value repetition\n\n\nData leakage\nReview data sources and pipeline; make sure target isn’t leaking into features\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nModel performs well on training, terrible on test (overfitting)\nReduce model complexity, add regularization, or get more data\n\n\nModel performs poorly on training AND test data (underfitting)\nUse a more complex model, add better features, reduce regularization.\n\n\nClassification model worse than a random guess or worse than majority class guess\nInvestigate data quality, imbalanced classes.\n\n\nModel performs unusually well on train and test data\nCheck for data leakage; the model may have access to information it shouldn’t.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nJupyter notebook is too large\nAvoid storing large Plotly outputs; clean outputs or split the notebook\n\n\nModel training takes too long\nUse smaller subsets for tuning; simplify the model or parallelize training",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "common_problems.html#data",
    "href": "common_problems.html#data",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nNot enough data\nUse data augmentation or synthetic sampling (e.g. SMOTE, SDV)\n\n\nData not representative of the distribution\nReassess how data was collected; consider stratified sampling\n\n\nImbalanced classes\nTry resampling, adjusting class weights, or adding synthetic data with SDV\n\n\nToo much data (examples)\nSubsample or use mini-batch training; profile before full-scale training\n\n\nToo many features / high dimensionality\nApply feature selection or dimensionality reduction (e.g. PCA)\n\n\nData has extreme values, outliers, or anomalies\nUse robust statistics, or find such values using outlier/anomaly detection methods. Consider removing examples.\n\n\nData may have been faked\nCheck for duplicate rows, unnatural distributions, and value repetition\n\n\nData leakage\nReview data sources and pipeline; make sure target isn’t leaking into features",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "common_problems.html#modeling",
    "href": "common_problems.html#modeling",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nModel performs well on training, terrible on test (overfitting)\nReduce model complexity, add regularization, or get more data\n\n\nModel performs poorly on training AND test data (underfitting)\nUse a more complex model, add better features, reduce regularization.\n\n\nClassification model worse than a random guess or worse than majority class guess\nInvestigate data quality, imbalanced classes.\n\n\nModel performs unusually well on train and test data\nCheck for data leakage; the model may have access to information it shouldn’t.",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "common_problems.html#workflow",
    "href": "common_problems.html#workflow",
    "title": "Common Problems in Data Science",
    "section": "",
    "text": "Problem\nSolution\n\n\n\n\nJupyter notebook is too large\nAvoid storing large Plotly outputs; clean outputs or split the notebook\n\n\nModel training takes too long\nUse smaller subsets for tuning; simplify the model or parallelize training",
    "crumbs": [
      "Common DS Problems"
    ]
  },
  {
    "objectID": "workflows/classification_tabular.html",
    "href": "workflows/classification_tabular.html",
    "title": "Classification with Tabular Data",
    "section": "",
    "text": "Classification with Tabular Data\nSupervised ML workflow for building a classification model on tabular data with categorical and continuous features.\nUsing Palmer’s penguins dataset from seaborn, train a random forest to predict the penguin species. We use scikit-learn for pre-processing, modeling, and evaluation.\nThis script:\n\nIncludes stratified train/test split\nIncludes imputation inside the pipeline\nHandles categorical and numerical features separately\nRuns a grid search to find best model parameters\nEvaluates results using both classification report and confusion matrix\nExtracts feature importances with proper naming\n\n\n\n\n\n\n\nNote: Using display for HTML tables\n\n\n\n\n\nprint(summarize(df)) and print(df.head()) return tables printed in plain text. To get nicer-formatted HTML tables, use the following instead of print():\nfrom IPython.display import display\ndisplay(df.head())\n\n# Display summary\ndisplay(summarize(df))\n\n\n\n\nfrom IPython.display import display\nimport seaborn as sns\nimport pandas as pd\nfrom minieda import summarize # pip install git+https://github.com/dbolotov/minieda.git\nimport time\nfrom pprint import pprint\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\n\npd.set_option(\"display.width\", 220) # set display width for printed tables\n\n# Load dataset and display first few rows\ndf = sns.load_dataset(\"penguins\")\n\nprint(\"----- SCRIPT OUTPUT -----\")\nprint(\"\\n----- First Few Rows of Data -----\\n\")\nprint(df.head())\n\n# Display summary\nprint(\"\\n----- Data Summary -----\\n\")\nprint(summarize(df))\n\n# Per-class value count\nprint(\"\\n----- Target class frequencies (normalized) -----\\n\")\nprint(df['species'].value_counts(normalize=True).rename(None).rename_axis(None))\n\n# Define columns\ncat_cols = ['island', 'sex']\nnum_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n\n# Drop rows where the target is missing (can't model without target)\ndf = df.dropna(subset=['species'])\n\n# Split the data\nX = df[cat_cols + num_cols]\ny = df['species']\n\n# Split into training and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Define preprocessing for numeric and categorical features\nnumeric_preprocessing = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler())\n])\n\ncategorical_preprocessing = Pipeline([\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('encode', OneHotEncoder(drop='first', sparse_output=False))  # one-hot; drop first feature to avoid multicollinearity\n])\n\n# Combine into a column transformer\npreprocessor = ColumnTransformer([\n    ('num', numeric_preprocessing, num_cols),\n    ('cat', categorical_preprocessing, cat_cols)\n])\n\n# Base pipeline\nclf_pipeline = Pipeline([\n    ('pre', preprocessor),\n    ('model', RandomForestClassifier(random_state=42))\n])\n\n# Define hyperparameter grid\nparam_grid = {\n    'model__n_estimators': [20,30,40],\n    'model__max_depth': [None],\n    'model__min_samples_leaf': [1, 3, 5, 7],\n    'model__max_features': ['sqrt']\n}\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    estimator=clf_pipeline,\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=2\n)\n\n# Fit grid search on training data\nprint(\"\\n----- GRID SEARCH -----\\n\")\nstart_time = time.time()\ngrid_search.fit(X_train, y_train)\nprint(f\"\\nGrid search completed in {time.time() - start_time:.2f} seconds\")\n\nprint(\"\\n----- Best Grid Search Result -----\")\nprint(f\"Accuracy: {grid_search.best_score_:.4f} ± {grid_search.cv_results_['std_test_score'][grid_search.best_index_]:.4f}\")\nprint(\"Parameters:\")\npprint(grid_search.best_params_)\n\n# Use best model from grid search\nclf_pipeline = grid_search.best_estimator_\n\n# Evaluate\nprint(\"\\n----- EVALUATION -----\")\nprint(\"\\n----- Train/Test Accuracy -----\\n\")\nprint(f\"Train accuracy: {clf_pipeline.score(X_train, y_train):.4f}\")\nprint(f\"Test accuracy:  {clf_pipeline.score(X_test, y_test):.4f}\")\n\ny_test_pred = clf_pipeline.predict(X_test)\nprint(\"\\n----- Classification Report -----\\n\")\nprint(classification_report(y_test, y_test_pred))\nprint(\"\\n----- Confusion Matrix -----\\n\")\ncm = confusion_matrix(y_test, y_test_pred, labels=clf_pipeline.classes_)\nprint(cm)\n\n# Print normalized feature importances\nmodel = clf_pipeline.named_steps['model']\nencoded_feature_names = clf_pipeline.named_steps['pre'].get_feature_names_out()\n\nfeat_importance_df = pd.DataFrame({\n    'feature': encoded_feature_names,\n    'importance': model.feature_importances_\n}).sort_values(by='importance', ascending=False)\n\nprint(\"\\n----- Feature Importance -----\\n\")\nprint(feat_importance_df)\n\n----- SCRIPT OUTPUT -----\n\n----- First Few Rows of Data -----\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1           18.7              181.0       3750.0    Male\n1  Adelie  Torgersen            39.5           17.4              186.0       3800.0  Female\n2  Adelie  Torgersen            40.3           18.0              195.0       3250.0  Female\n3  Adelie  Torgersen             NaN            NaN                NaN          NaN     NaN\n4  Adelie  Torgersen            36.7           19.3              193.0       3450.0  Female\n\n----- Data Summary -----\n\n                     dtype  count  unique  unique_perc  missing  missing_perc  zero  zero_perc     top freq     mean     std     min     50%     max  skew\nbill_length_mm     float64    342     164        47.67        2          0.58     0        0.0                 43.92    5.46    32.1   44.45    59.6  0.05\nbill_depth_mm      float64    342      80        23.26        2          0.58     0        0.0                 17.15    1.97    13.1    17.3    21.5 -0.14\nflipper_length_mm  float64    342      55        15.99        2          0.58     0        0.0                200.92   14.06   172.0   197.0   231.0  0.35\nbody_mass_g        float64    342      94        27.33        2          0.58     0        0.0               4201.75  801.95  2700.0  4050.0  6300.0  0.47\nspecies             object    344       3         0.87        0          0.00     0        0.0  Adelie  152                                               \nisland              object    344       3         0.87        0          0.00     0        0.0  Biscoe  168                                               \nsex                 object    333       2         0.58       11          3.20     0        0.0    Male  168                                               \n\n----- Target class frequencies (normalized) -----\n\nAdelie       0.441860\nGentoo       0.360465\nChinstrap    0.197674\ndtype: float64\n\n----- GRID SEARCH -----\n\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n\nGrid search completed in 2.90 seconds\n\n----- Best Grid Search Result -----\nAccuracy: 0.9855 ± 0.0073\nParameters:\n{'model__max_depth': None,\n 'model__max_features': 'sqrt',\n 'model__min_samples_leaf': 1,\n 'model__n_estimators': 30}\n\n----- EVALUATION -----\n\n----- Train/Test Accuracy -----\n\nTrain accuracy: 1.0000\nTest accuracy:  1.0000\n\n----- Classification Report -----\n\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n----- Confusion Matrix -----\n\n[[30  0  0]\n [ 0 14  0]\n [ 0  0 25]]\n\n----- Feature Importance -----\n\n                  feature  importance\n0     num__bill_length_mm    0.312770\n1      num__bill_depth_mm    0.216133\n2  num__flipper_length_mm    0.205913\n4       cat__island_Dream    0.138414\n3        num__body_mass_g    0.096331\n5   cat__island_Torgersen    0.021391\n6           cat__sex_Male    0.009048",
    "crumbs": [
      "Workflows",
      "Classification"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\nOnline Books\n\n\n\n\nCategory\nResource\nDescription\n\n\n\n\nExplainability\nInterpretable Machine Learning\nA practical overview of techniques for making ML models more transparent, including SHAP.\n\n\nVisualization\nUW Interactive Data Lab Curriculum\nBook on statistical visualization using Vega-Lite and Altair.\n\n\nVisualization\nFundamentals of Data Visualization\nPrinciples and examples of clear, effective visual communication.\n\n\nTime Series\nForecasting: Principles and Practice\nCovers forecasting techniques like exponential smoothing and ARIMA, with examples in R.\n\n\nData Imputation\nFlexible Imputation of Missing Data\nMethods to handle missing data, with emphasis on multiple imputation.\n\n\nFraud Detection\nFraud Detection Handbook\nApplied techniques for detecting fraud in highly imbalanced datasets. Includes instructions on using a fraud data simulator.\n\n\n\n\n\n\nTools\n\nSDV - Python library for creating tabular synthetic data.\npermetrics - Python library for performance metrics of machine learning models. Documentation site includes quick explanations of each metric.\n\n\n\nResources used to make this guide\n\nQuarto: Extensive publishing system. Supports jupyter notebooks and markdown.\nbootswatch: Collection of free themes for Bootstrap-based sites.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "concepts/intervals.html",
    "href": "concepts/intervals.html",
    "title": "Confidence and Prediction Intervals",
    "section": "",
    "text": "Machine learning models typically return point predictions, not uncertainty estimates. However, intervals are useful when presenting results or identifying low-confidence cases. Here we discuss two types of intervals:\nThese intervals are common in statistical modeling and in time series forecasting, but less so in machine learning because many ML models don’t compute them directly. One workaround is bootstrapping, where multiple models are fit on resampled training data, and the spread of predictions is used to estimate uncertainty. In this case, intervals are estimated empirically rather than derived analytically.",
    "crumbs": [
      "Concepts",
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "concepts/intervals.html#example",
    "href": "concepts/intervals.html#example",
    "title": "Confidence and Prediction Intervals",
    "section": "Example",
    "text": "Example\nSuppose we’re building a model to predict daily rainfall (in millimeters) based only on humidity. Let’s say that for the input of {'humidity': 85}, the model returns:\nPredicted rainfall (mm): 3.8  \n95% confidence interval (mm): [3.5, 4.1]  \n95% prediction interval (mm): [1.2, 6.4]\nIn this case:\n\nThe confidence interval says where the average rainfall would fall across many days with similar humidity.\nThe prediction interval says where the rainfall for a single day with that humidity might fall, accounting for noise in the data and variation not captured by the model.",
    "crumbs": [
      "Concepts",
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "concepts/intervals.html#visualization",
    "href": "concepts/intervals.html#visualization",
    "title": "Confidence and Prediction Intervals",
    "section": "Visualization",
    "text": "Visualization\nThe example below uses linear regression to show both intervals for a model that predicts rainfall from humidity (from a simulated dataset). The prediction intervals are wider than the confidence intervals.\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Simulate data: rainfall increases with humidity with some noise\nnp.random.seed(1)\nhumidity = np.random.uniform(40, 100, 100)\nrainfall = 0.1 * humidity + np.random.normal(0, 2, size=100)\n\n# Fit linear model\nX = sm.add_constant(humidity)\nmodel = sm.OLS(rainfall, X).fit()\n\n# Generate prediction range\nhumidity_pred = np.linspace(40, 100, 100)\nX_pred = sm.add_constant(humidity_pred)\npreds = model.get_prediction(X_pred).summary_frame(alpha=0.05)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.scatter(humidity, rainfall, alpha=0.5, label=\"Observed data\")\nplt.plot(humidity_pred, preds[\"mean\"], color=\"black\", label=\"Predicted (mean)\")\nplt.fill_between(humidity_pred, preds[\"mean_ci_lower\"], preds[\"mean_ci_upper\"],\n                 color=\"steelblue\", alpha=0.3, label=\"95% Confidence Interval\")\nplt.fill_between(humidity_pred, preds[\"obs_ci_lower\"], preds[\"obs_ci_upper\"],\n                 color=\"orange\", alpha=0.2, label=\"95% Prediction Interval\")\n\nplt.xlabel(\"Humidity (%)\")\nplt.ylabel(\"Rainfall (mm)\")\nplt.title(\"Confidence vs. Prediction Intervals (Predicting Rainfall from Humidity)\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Concepts",
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\n\nExplainability\nOversampling\nOverfitting and Underfitting\nData leakage\n\nExamples\n\nImputing missing values on the entire dataset before splitting into train and test.\n\n\nTrain, test, validation sets\nFeatures, columns, predictors\nTypes of learning\n\nSupervised learning\nUnsupervised learning\nDeep learning\nReinforcement learning\n\nDescriptive statistics\nDistribution\nRegularization\n\nRidge\nLasso\nIn neural networks",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Quick Ref",
    "section": "",
    "text": "This site is a concise, opinionated reference for practical data science and machine learning. It’s built for students and professionals looking for summaries of key ideas, workflows, and tools.\nAll the code examples are in Python, using libraries like pandas, scikit-learn, and matplotlib. They are written to be easy to copy and adapt.\nThis is a work in progress, made by a data scientist who got tired of looking this stuff frequently :)\n\n\n\n\nUse the left sidebar to browse topics\n\nUse the search bar to find specific terms or methods\n\n\n\n\n Author: Dmitriy Bolotov | dbolotov.github.io",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Data Science Quick Ref",
    "section": "",
    "text": "This site is a concise, opinionated reference for practical data science and machine learning. It’s built for students and professionals looking for summaries of key ideas, workflows, and tools.\nAll the code examples are in Python, using libraries like pandas, scikit-learn, and matplotlib. They are written to be easy to copy and adapt.\nThis is a work in progress, made by a data scientist who got tired of looking this stuff frequently :)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Data Science Quick Ref",
    "section": "",
    "text": "Use the left sidebar to browse topics\n\nUse the search bar to find specific terms or methods",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Data Science Quick Ref",
    "section": "",
    "text": "Author: Dmitriy Bolotov | dbolotov.github.io",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "concepts/metrics.html",
    "href": "concepts/metrics.html",
    "title": "Data Science and Machine Learning Metrics",
    "section": "",
    "text": "This page is a quick reference for common metrics across tasks like classification, regression, and clustering. Each entry includes a definition, when to use it, and links to an explanation (mostly from Wikipedia) and the relevant scikit-learn doc. This list is not meant to be exhaustive.\n\n\nA metric is a number that measures model performance, or how well predictions match actual outcomes.\n\nIn supervised learning, metrics evaluate prediction quality (e.g. RMSE, F1).\nIn unsupervised learning, they assess structure or similarity (e.g. silhouette score).\nDuring training, metrics guide choices like model selection and early stopping.\n\n\n\n\n\nLoss function: What the model optimizes during training.\nMetric: What you monitor to evaluate results.\nError metric: Often used in regression to describe prediction error.\n\n\n\n\n\n\n\n\n\n\nMetric\nCode\nDetails\n\n\n\n\nAccuracy\nskl\nAccuracy. Proportion of correct predictions to total predictions. Simple and intuitive. Can be very misleading for imbalanced classes.\n\n\nPrecision\nskl\nPrecision. True Positives / (True Positives + False Positives). How many predicted positives are correct.\n\n\nRecall\nskl\nRecall. True Positives / (True Positives + False Negatives). How many actual positives were captured.\n\n\nF1\nskl\nF1 Score. Harmonic mean of precision and recall. Good for imbalanced classes.\n\n\nROC AUC\nskl\nROC AUC. Area under the ROC curve. Evaluates ranking performance across thresholds.\n\n\nPR AUC\nskl\nPR AUC. Area under the Precision-Recall curve. Better than ROC AUC for rare positives.\n\n\nLog Loss\nskl\nLogarithmic Loss. Penalizes confident wrong predictions. Common in probabilistic classifiers.\n\n\nBalanced Acc\nskl\nBalanced Accuracy. Mean recall across classes. Helps with imbalanced classes.\n\n\nMCC\nskl\nMatthews Correlation Coefficient Balanced score even for class imbalance. Based on confusion matrix.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nCode\nDetails\n\n\n\n\nMSE\nskl\nMean Squared Error. Average squared difference between predictions and true values. Penalizes larger errors more; sensitive to outliers. Not in original units.\n\n\nRMSE\nskl\nRoot Mean Squared Error. Same as MSE but in the original unit scale; easier to interpret. Still sensitive to outliers.\n\n\nMAE\nskl\nMean Absolute Error. Average absolute difference between predictions and actual values. More robust to outliers than MSE.\n\n\nR²\nskl\nCoefficient of Determination. Measures proportion of variance explained by the model. Can be negative.\n\n\nAdj R²\n\nAdjusted R². Like R² but penalizes for additional predictors. Helps avoid overfitting.\n\n\nMSLE\nskl\nMean Squared Log Error. MSE on log-transformed targets. Good for targets spanning orders of magnitude.\n\n\nMAPE\nskl\nMean Absolute Percentage Error. Average of absolute percentage errors. Can blow up if targets are near zero.\n\n\nSMAPE\n\nSymmetric MAPE. Like MAPE but less sensitive to small denominators. Often used in time series.",
    "crumbs": [
      "Concepts",
      "Metrics"
    ]
  },
  {
    "objectID": "concepts/metrics.html#metrics-explained",
    "href": "concepts/metrics.html#metrics-explained",
    "title": "Data Science and Machine Learning Metrics",
    "section": "",
    "text": "A metric is a number that measures model performance, or how well predictions match actual outcomes.\n\nIn supervised learning, metrics evaluate prediction quality (e.g. RMSE, F1).\nIn unsupervised learning, they assess structure or similarity (e.g. silhouette score).\nDuring training, metrics guide choices like model selection and early stopping.",
    "crumbs": [
      "Concepts",
      "Metrics"
    ]
  },
  {
    "objectID": "concepts/metrics.html#related-terms",
    "href": "concepts/metrics.html#related-terms",
    "title": "Data Science and Machine Learning Metrics",
    "section": "",
    "text": "Loss function: What the model optimizes during training.\nMetric: What you monitor to evaluate results.\nError metric: Often used in regression to describe prediction error.",
    "crumbs": [
      "Concepts",
      "Metrics"
    ]
  },
  {
    "objectID": "concepts/metrics.html#metrics-tables",
    "href": "concepts/metrics.html#metrics-tables",
    "title": "Data Science and Machine Learning Metrics",
    "section": "",
    "text": "Metric\nCode\nDetails\n\n\n\n\nAccuracy\nskl\nAccuracy. Proportion of correct predictions to total predictions. Simple and intuitive. Can be very misleading for imbalanced classes.\n\n\nPrecision\nskl\nPrecision. True Positives / (True Positives + False Positives). How many predicted positives are correct.\n\n\nRecall\nskl\nRecall. True Positives / (True Positives + False Negatives). How many actual positives were captured.\n\n\nF1\nskl\nF1 Score. Harmonic mean of precision and recall. Good for imbalanced classes.\n\n\nROC AUC\nskl\nROC AUC. Area under the ROC curve. Evaluates ranking performance across thresholds.\n\n\nPR AUC\nskl\nPR AUC. Area under the Precision-Recall curve. Better than ROC AUC for rare positives.\n\n\nLog Loss\nskl\nLogarithmic Loss. Penalizes confident wrong predictions. Common in probabilistic classifiers.\n\n\nBalanced Acc\nskl\nBalanced Accuracy. Mean recall across classes. Helps with imbalanced classes.\n\n\nMCC\nskl\nMatthews Correlation Coefficient Balanced score even for class imbalance. Based on confusion matrix.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nCode\nDetails\n\n\n\n\nMSE\nskl\nMean Squared Error. Average squared difference between predictions and true values. Penalizes larger errors more; sensitive to outliers. Not in original units.\n\n\nRMSE\nskl\nRoot Mean Squared Error. Same as MSE but in the original unit scale; easier to interpret. Still sensitive to outliers.\n\n\nMAE\nskl\nMean Absolute Error. Average absolute difference between predictions and actual values. More robust to outliers than MSE.\n\n\nR²\nskl\nCoefficient of Determination. Measures proportion of variance explained by the model. Can be negative.\n\n\nAdj R²\n\nAdjusted R². Like R² but penalizes for additional predictors. Helps avoid overfitting.\n\n\nMSLE\nskl\nMean Squared Log Error. MSE on log-transformed targets. Good for targets spanning orders of magnitude.\n\n\nMAPE\nskl\nMean Absolute Percentage Error. Average of absolute percentage errors. Can blow up if targets are near zero.\n\n\nSMAPE\n\nSymmetric MAPE. Like MAPE but less sensitive to small denominators. Often used in time series.",
    "crumbs": [
      "Concepts",
      "Metrics"
    ]
  },
  {
    "objectID": "workflows/index.html",
    "href": "workflows/index.html",
    "title": "Workflows Index",
    "section": "",
    "text": "Workflows Index\nEnd-to-end Jupyter Notebook templates for common data science tasks. Each example uses a small, built-in dataset.\n\nClassification with Tabular Data - Training a random forest classifier with scikit-learn preprocessing and grid search on the Palmer penguins dataset.\nRegression with Tabular Data - Training different regression models with scikit-learn preprocessing and grid search on the tips dataset.",
    "crumbs": [
      "Workflows",
      "Workflows Index"
    ]
  },
  {
    "objectID": "workflows/regression_tabular.html",
    "href": "workflows/regression_tabular.html",
    "title": "Regression with Tabular Data",
    "section": "",
    "text": "Regression with Tabular Data\nSupervised ML workflow for building a regression model on tabular data with categorical and continuous features.\nUses the tips dataset from seaborn, and trains several models to predict the tip amount. Uses scikit-learn for pre-processing, modeling, and evaluation.\nThis dataset contains no missing values, so imputation is not used.\nThis script:\n\nIncludes train/test split\nHandles categorical and numerical features separately\nRuns a grid search to find best model parameters for several models: linear regression, lasso, ridge, random forest regressor, knn regressor, xgboost regressor\nEvaluates results using RMSE, MAE, R² score\nCreates 3 plots for evaluating residuals per model\nPredicts target on a fictional example\n\n\n\n\n\n\n\nNote: Using display for fancier tables\n\n\n\n\n\nprint(summarize(df)) and print(df.head()) return tables printed in plain text. To get nicer-formatted HTML tables, use the following instead of print():\nfrom IPython.display import display\ndisplay(df.head())\n\n# Display summary\ndisplay(summarize(df))\n\n\n\n\nimport seaborn as sns\nimport pandas as pd\nfrom minieda import summarize # pip install git+https://github.com/dbolotov/minieda.git\nimport time\nfrom pprint import pprint\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nplt.rcParams.update({'font.size': 9}) # set global plot params\npd.set_option(\"display.width\", 220) # set display width for printed tables\n\n# Load dataset and display first few rows\ndf = sns.load_dataset(\"tips\")\n\nprint(\"----- SCRIPT OUTPUT -----\")\nprint(\"\\n----- First Few Rows of Data -----\\n\")\nprint(df.head())\n\n# Display summary\nprint(\"\\n----- Data Summary -----\\n\")\nprint(summarize(df))\n\n# Define columns\ncat_cols = ['sex', 'smoker', 'day', 'time']\nnum_cols = ['total_bill', 'size']\n\n# Split the data\nX = df[cat_cols + num_cols]\ny = df['tip']\n\n# Split into training and test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define preprocessing for numeric and categorical features\nnumeric_preprocessing = Pipeline([\n    ('scale', StandardScaler())\n])\n\ncategorical_preprocessing = Pipeline([\n    ('encode', OneHotEncoder(drop='first', sparse_output=False))  # one-hot; drop first feature to avoid multicollinearity\n])\n\n# Combine into a column transformer\npreprocessor = ColumnTransformer([\n    ('num', numeric_preprocessing, num_cols),\n    ('cat', categorical_preprocessing, cat_cols)\n])\n\n# Define models and hyperparameters to optimize\nmodels = [\n    {\n        \"name\": \"LinearRegression\",\n        \"estimator\": LinearRegression(),\n        \"param_grid\": None\n    },\n    {\n        \"name\": \"Ridge\",\n        \"estimator\": Ridge(),\n        \"param_grid\": {\n            \"model__alpha\": [0.1, 1.0, 10.0]\n        }\n    },\n    {\n        \"name\": \"Lasso\",\n        \"estimator\": Lasso(),\n        \"param_grid\": {\n            \"model__alpha\": [0.1, 1.0, 10.0]\n        }\n    },\n    {\n        \"name\": \"RandomForest\",\n        \"estimator\": RandomForestRegressor(random_state=42),\n        \"param_grid\": {\n            \"model__n_estimators\": [20,30],\n            \"model__max_depth\": [None, 10],\n            \"model__min_samples_split\": [2, 5]\n        }\n    },\n    {\n        \"name\": \"KNN\",\n        \"estimator\": KNeighborsRegressor(),\n        \"param_grid\": {\n            \"model__n_neighbors\": [3, 5, 7]\n        }\n    },\n    {\n        \"name\": \"XGBoost\",\n        \"estimator\": XGBRegressor(random_state=42, verbosity=0),\n        \"param_grid\": {\n            \"model__n_estimators\": [30, 50],\n            \"model__learning_rate\": [0.1, 0.3],\n            \"model__max_depth\": [3, 6]\n        }\n    }\n]\n\nresults = []\n\nprint(\"\\n----- GRID SEARCH WITH BEST RESULT ON TRAIN AND TEST SETS -----\")\n\nstart_time = time.time()\ntrained_models = {}\nfor m in models:\n    print(f\"\\n--- {m['name']} ---\")\n    \n    pipeline = Pipeline([('pre', preprocessor), ('model', m[\"estimator\"])])\n\n    if m[\"param_grid\"]:\n        search = GridSearchCV(\n            pipeline,\n            m[\"param_grid\"],\n            scoring=\"neg_root_mean_squared_error\",  # RMSE\n            cv=3,\n            n_jobs=-1,\n            verbose=1\n        )\n        search.fit(X_train, y_train)\n        best_model = search.best_estimator_\n        print(\"Best params:\", search.best_params_)\n    else: #if no parameters to optimize\n        pipeline.fit(X_train, y_train)\n        best_model = pipeline\n    \n    trained_models[m[\"name\"]] = best_model\n\n    # Predict and evaluate on both train and test\n    y_train_pred = best_model.predict(X_train)\n    y_test_pred = best_model.predict(X_test)\n    \n    train_rmse = root_mean_squared_error(y_train, y_train_pred)\n    test_rmse = root_mean_squared_error(y_test, y_test_pred)\n    \n    train_mae = mean_absolute_error(y_train, y_train_pred)\n    test_mae = mean_absolute_error(y_test, y_test_pred)\n    \n    train_r2 = r2_score(y_train, y_train_pred)\n    test_r2 = r2_score(y_test, y_test_pred)\n    \n    print(\"\")\n    print(f\"{'Metric':&lt;6} | {'Train':&gt;6} | {'Test':&gt;6}\")\n    # print(\"-\" * 24)\n    print(f\"{'RMSE':&lt;6} | {train_rmse:&gt;6.4f} | {test_rmse:&gt;6.4f}\")\n    print(f\"{'MAE':&lt;6} | {train_mae:&gt;6.4f} | {test_mae:&gt;6.4f}\")\n    print(f\"{'R²':&lt;6} | {train_r2:&gt;6.4f} | {test_r2:&gt;6.4f}\")\n\n    # results.append({\n    #     \"model\": m[\"name\"],\n    #     \"rmse\": rmse,\n    #     \"mae\": mae,\n    #     \"r2\": r2\n    # })\n\n    results.append({\n        \"model\": m[\"name\"],\n        \"train_rmse\": train_rmse,\n        \"test_rmse\": test_rmse,\n        \"train_mae\": train_mae,\n        \"test_mae\": test_mae,\n        \"train_r2\": train_r2,\n        \"test_r2\": test_r2\n    })\n\nprint(f\"\\nGrid search completed in {time.time() - start_time:.2f} seconds\")\n\nprint(\"\\n----- EVALUATION SUMMARY -----\\n\")\nprint(pd.DataFrame(results).sort_values(\"test_rmse\", ascending=False))\nprint(\"\\n\")\n\ndef plot_residual_diagnostics(y_true, y_pred, model_name=\"Model\"):\n    residuals = y_true - y_pred\n\n    fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n    fig.suptitle(f\"Residual Diagnostics: {model_name}\")\n\n    # 1. Residuals vs Predicted\n    sns.scatterplot(x=y_pred, y=residuals, ax=axes[0])\n    axes[0].axhline(0, color=\"red\", linestyle=\"--\")\n    axes[0].set_xlabel(\"Predicted\")\n    axes[0].set_ylabel(\"Residuals\")\n    axes[0].set_title(\"Residuals vs Predicted\")\n\n    # 2. Histogram of residuals\n    sns.histplot(residuals, kde=True, ax=axes[1])\n    axes[1].set_title(\"Residuals Histogram\")\n    axes[1].set_xlabel(\"Residual\")\n\n    # 3. Q-Q Plot\n    stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n    axes[2].set_title(\"Q-Q Plot\")\n    axes[2].get_lines()[0].set_color(\"tab:blue\")  # points\n    axes[2].get_lines()[1].set_color(\"red\")       # reference line\n\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()\n\ny_test_pred = trained_models[\"RandomForest\"].predict(X_test)\nplot_residual_diagnostics(y_test, y_test_pred, model_name=\"Random Forest\")\n\n\n# Predict on a new example\nnew_sample = pd.DataFrame([{\n    'sex': 'Female',\n    'smoker': 'No',\n    'day': 'Sun',\n    'time': 'Dinner',\n    'total_bill': 40.0,\n    'size': 2\n}])\n\n# Predict on fictional sample with each model\npredictions = []\n\nfor model_name, model in trained_models.items():\n    pred = model.predict(new_sample)[0]\n    predictions.append({\"model\": model_name, \"predicted_tip\": round(pred, 2)})\n\n# Display predictions\nprint(\"----- PREDICTIONS ON NEW SAMPLE -----\")\nprint(\"\\n----- Sample Input -----\")\nprint(new_sample)\nprint(\"\\n----- Sample Predictions -----\\n\")\nprint(pd.DataFrame(predictions).sort_values(\"predicted_tip\", ascending=False))\n\n----- SCRIPT OUTPUT -----\n\n----- First Few Rows of Data -----\n\n   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n\n----- Data Summary -----\n\n               dtype  count  unique  unique_perc  missing  missing_perc  zero  zero_perc     top freq   mean   std   min   50%    max  skew\ntotal_bill   float64    244     229        93.85        0           0.0     0        0.0               19.79   8.9  3.07  17.8  50.81  1.13\ntip          float64    244     123        50.41        0           0.0     0        0.0                 3.0  1.38   1.0   2.9   10.0  1.47\nsize           int64    244       6         2.46        0           0.0     0        0.0                2.57  0.95   1.0   2.0    6.0  1.45\nsex         category    244       2         0.82        0           0.0     0        0.0    Male  157                                      \nsmoker      category    244       2         0.82        0           0.0     0        0.0      No  151                                      \nday         category    244       4         1.64        0           0.0     0        0.0     Sat   87                                      \ntime        category    244       2         0.82        0           0.0     0        0.0  Dinner  176                                      \n\n----- GRID SEARCH WITH BEST RESULT ON TRAIN AND TEST SETS -----\n\n--- LinearRegression ---\n\nMetric |  Train |   Test\nRMSE   | 1.0491 | 0.8387\nMAE    | 0.7600 | 0.6671\nR²     | 0.4582 | 0.4373\n\n--- Ridge ---\nFitting 3 folds for each of 3 candidates, totalling 9 fits\nBest params: {'model__alpha': 10.0}\n\nMetric |  Train |   Test\nRMSE   | 1.0507 | 0.8283\nMAE    | 0.7623 | 0.6657\nR²     | 0.4567 | 0.4511\n\n--- Lasso ---\nFitting 3 folds for each of 3 candidates, totalling 9 fits\nBest params: {'model__alpha': 0.1}\n\nMetric |  Train |   Test\nRMSE   | 1.0615 | 0.7824\nMAE    | 0.7777 | 0.6548\nR²     | 0.4454 | 0.5102\n\n--- RandomForest ---\nFitting 3 folds for each of 8 candidates, totalling 24 fits\nBest params: {'model__max_depth': 10, 'model__min_samples_split': 2, 'model__n_estimators': 30}\n\nMetric |  Train |   Test\nRMSE   | 0.4436 | 0.9459\nMAE    | 0.3313 | 0.7394\nR²     | 0.9032 | 0.2842\n\n--- KNN ---\nFitting 3 folds for each of 3 candidates, totalling 9 fits\nBest params: {'model__n_neighbors': 5}\n\nMetric |  Train |   Test\nRMSE   | 0.9195 | 0.9025\nMAE    | 0.6846 | 0.7160\nR²     | 0.5839 | 0.3484\n\n--- XGBoost ---\nFitting 3 folds for each of 8 candidates, totalling 24 fits\nBest params: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 30}\n\nMetric |  Train |   Test\nRMSE   | 0.7910 | 0.8755\nMAE    | 0.5830 | 0.7102\nR²     | 0.6920 | 0.3868\n\nGrid search completed in 2.66 seconds\n\n----- EVALUATION SUMMARY -----\n\n              model  train_rmse  test_rmse  train_mae  test_mae  train_r2   test_r2\n3      RandomForest    0.443583   0.945898   0.331275  0.739375  0.903152  0.284205\n4               KNN    0.919482   0.902499   0.684636  0.716000  0.583873  0.348381\n5           XGBoost    0.791011   0.875502   0.583020  0.710206  0.692033  0.386783\n0  LinearRegression    1.049133   0.838664   0.759965  0.667133  0.458249  0.437302\n1             Ridge    1.050658   0.828321   0.762260  0.665704  0.456673  0.451095\n2             Lasso    1.061543   0.782438   0.777739  0.654809  0.445356  0.510221\n\n\n----- PREDICTIONS ON NEW SAMPLE -----\n\n----- Sample Input -----\n      sex smoker  day    time  total_bill  size\n0  Female     No  Sun  Dinner        40.0     2\n\n----- Sample Predictions -----\n\n              model  predicted_tip\n0  LinearRegression           4.93\n3      RandomForest           4.78\n1             Ridge           4.77\n2             Lasso           4.63\n5           XGBoost           4.39\n4               KNN           3.50",
    "crumbs": [
      "Workflows",
      "Regression"
    ]
  }
]